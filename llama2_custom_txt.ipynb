{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPVaE9emBbq9tMBwf0rhUWs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JZ2RFyafE2uw"},"outputs":[],"source":["!pip install GPUtil\n","\n","import torch\n","import GPUtil\n","import os\n","\n","GPUtil.showUtilization()\n","\n","if torch.cuda.is_available():\n","    print(\"GPU is available!\")\n","else:\n","    print(\"GPU not available.\")\n","\n","os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the GPU ID (0 for T4)\n","\n","!nvidia-smi"]},{"cell_type":"code","source":["!pip install -q git+https://github.com/huggingface/peft.git\n","!pip install -q -i https://pypi.org/simple/ bitsandbytes\n","!pip install -q transformers==4.30\n","!pip install -q datasets\n","!pip install pyarrow\n","\n","import torch\n","import transformers\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers import BitsAndBytesConfig,LlamaTokenizer\n","from huggingface_hub import notebook_login\n","from datasets import load_dataset\n","from peft import prepare_model_for_kbit_training\n","from peft import LoraConfig, get_peft_model\n","from datetime import datetime\n","\n","if 'COLAB_GPU' in os.environ:\n","    from google.colab import output\n","    output.enable_custom_widget_manager()"],"metadata":{"id":"5moGLtptE84p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'COLAB_GPU' in os.environ:\n","    !huggingface-cli login\n","else:\n","    notebook_login()"],"metadata":{"id":"jk8J9tSdFAH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model_id,\n","                                             quantization_config=bnb_config)"],"metadata":{"id":"s37gVx5YFBZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dir_path = '/workspace/txt/'\n","\n","os.chdir(dir_path)\n","\n","# train_dataset = load_dataset(\"text\", data_files={\"train\":\n","#                 [\"bluetooth6.0.0.txt\", \"bluetooth6.1.0.txt\",\n","#                  \"bluetooth7.0.0.txt\",\"bluetooth7.0.1.txt\",\n","#                  \"bluetooth7.1.1.txt\",\"bluetooth8.0.0.txt\",\n","#                  \"bluetooth8.1.0.txt\",\"bluetooth7.1.0.txt\"]}, split='train')\n","\n","train_dataset = load_dataset(\"text\", data_files={\"train\":\n","                [\"bluetooth8.1.0.txt\"]}, split='train')\n","\n","os.chdir('..')\n","\n","# drive 연결하니까 /content 까지."],"metadata":{"id":"zWeRXdtLFEC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n","                                           trust_remote_code=True,\n","                                           add_eos_token=True)\n","\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","# set the pad token to indicate that it's the end-of-sentence\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"XdX9hbDpFLso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_train_dataset=[]\n","for phrase in train_dataset:\n","    tokenized_train_dataset.append(tokenizer(phrase['text']))"],"metadata":{"id":"WKxac8TKFNCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# gradient checkpointing to reduce memory usage for increased compute time\n","model.gradient_checkpointing_enable()\n","\n","# compressing the base model into a smaller, more efficient model\n","model = prepare_model_for_kbit_training(model)"],"metadata":{"id":"cZcxYyMKFOOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = LoraConfig(\n","    # rank of the update matrices\n","    # Lower rank results in smaller matrices with fewer trainable params\n","    r=8,\n","\n","    # impacts low-rank approximation aggressiveness\n","    # increasing value speeds up training\n","    lora_alpha=64,\n","\n","    # modules to apply the LoRA update matrices\n","    target_modules=[\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"gate_proj\",\n","        \"down_proj\",\n","        \"up_proj\",\n","        \"o_proj\"\n","    ],\n","\n","    # determines LoRA bias type, influencing training dynamics\n","    bias=\"none\",\n","\n","    # regulates model regularization; increasing may lead to underfitting\n","    lora_dropout=0.1,\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model = get_peft_model(model, config)"],"metadata":{"id":"hxkR4afkFP2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TWO IMPORTANT TRAINING PARAMETERS TO CONSIDER CHANGING\n","\n","train_epochs_val = 5 # CHANGE VALUE AS NEEDED HERE!\n","'''\n","train_epochs_val is the times the model will iterate over the entire training\n","dataset. Increasing the value may allow the model to learn more from the data,\n","but be cautious of overfitting.\n","'''\n","\n","learning_rate_val = 1e-4 # CHANGE VALUE AS NEEDED HERE!\n","'''\n","A higher learning_rate_val can lead to faster convergence, but it might\n","overshoot the optimal solution. Conversely, a lower value may result\n","in slower training but better fine-tuning.\n","'''\n","\n","\n","trainer = transformers.Trainer(\n","    model=model,                             # llama-2-7b-chat model\n","    train_dataset=tokenized_train_dataset,   # training data that's tokenized\n","    args=transformers.TrainingArguments(\n","        output_dir=\"./finetunedModel\",       # directory where checkpoints are saved\n","        per_device_train_batch_size=2,       # number of samples processed in one forward/backward pass per GPU\n","        gradient_accumulation_steps=2,       # [default = 1] number of updates steps to accumulate the gradients for\n","        num_train_epochs=train_epochs_val,   # [IMPORTANT] number of times of complete pass through the entire training dataset\n","        learning_rate=learning_rate_val,     # [IMPORTANT] smaller LR for better finetuning\n","        bf16=False,                          # train parameters with this precision\n","        optim=\"paged_adamw_8bit\",            # use paging to improve memory management of default adamw optimizer\n","        logging_dir=\"./logs\",                # directory to save training log outputs\n","        save_strategy=\"epoch\",               # [default = \"steps\"] store after every iteration of a datapoint\n","        save_steps=50,                       # save checkpoint after number of iterations\n","        logging_steps = 10                   # specify frequency of printing training loss data\n","    ),\n","\n","    # use to form a batch from a list of elements of train_dataset\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","\n","# if use_cache is True, past key values are used to speed up decoding\n","# if applicable to model. This defeats the purpose of finetuning\n","model.config.use_cache = False\n","\n","# train the model based on the above config\n","trainer.train()"],"metadata":{"id":"AwV3Y5y8FRqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers import BitsAndBytesConfig,LlamaTokenizer\n","from peft import PeftModel\n","\n","base_model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n","\n","nf4Config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","# nf8Config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold = 6)\n","\n","tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n","                                           trust_remote_code=True,\n","                                           add_eos_token=True)\n","\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,  #same as before\n","    quantization_config=nf4Config,  #same quantization config as before\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    use_auth_token=True\n",")\n","\n","tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False,\n","                                           trust_remote_code=True)\n","\n","# Change model checkpoint that has least training loss in the code below\n","# beware of overfitting!\n","modelFinetuned = PeftModel.from_pretrained(base_model,\"finetunedModel/checkpoint-29804\")"],"metadata":{"id":"1wYBDrcHFT9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ENTER YOUR QUESTION BELOW\n","\n","question = \"Just answer this question: I want to find bluetooth related function that have [in], [out] parameters. Write the function names.\"\n","\n","# Format the question\n","eval_prompt = f\"{question}\\n\\n\"\n","\n","promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","modelFinetuned.eval()\n","with torch.no_grad():\n","    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n","torch.cuda.empty_cache()"],"metadata":{"id":"Neg20V1TFVr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### ENTER YOUR QUESTION BELOW\n","\n","question = \"Just answer this question: Write the fuction names that have [in], [out] parameters.\"\n","\n","# Format the question\n","eval_prompt = f\"{question}\\n\\n\"\n","\n","promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","modelFinetuned.eval()\n","with torch.no_grad():\n","    print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens = 1024)[0], skip_special_tokens=True))\n","torch.cuda.empty_cache()"],"metadata":{"id":"rOr6EvBNFXGm"},"execution_count":null,"outputs":[]}]}